#!/usr/bin/env python3
"""
yeast-agent v0.3.0 - AI Proto-Consciousness Agent (Phase 3: Soft Autonomy)

Phase 3 adds:
- Self-modification proposals (Semantic, Tension, Maintenance)
- Proposal queue and logging
- "Soft Autonomy" without direct execution

All persistence lives outside the stateless LLM.
"""

import json
import sys
import os
import subprocess
import uuid
import re
import math
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional

def load_config_from_env() -> None:
    """Load configuration from .env file if it exists."""
    global MEMORY_DECAY_HALF_LIFE_DAYS
    
    env_path = Path.home() / ".env"
    if env_path.exists():
        try:
            with open(env_path, 'r') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        if '=' in line:
                            key, value = line.split('=', 1)
                            key = key.strip()
                            value = value.strip().strip('"')
                            
                            if key == "MEMORY_DECAY_HALF_LIFE_DAYS":
                                try:
                                    MEMORY_DECAY_HALF_LIFE_DAYS = float(value)
                                except ValueError:
                                    pass  # Keep default if not a valid number
        except Exception as e:
            pass  # Silently fail if env file can't be read

# ============================================================================ 
# Configuration

# ============================================================================ 

MEMORY_DIR = Path.home() / "yeast-data"
OLLAMA_API_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL = "mistral"

# Memory store structure
EPISODIC_DIR = MEMORY_DIR / "episodic"
SEMANTIC_DIR = MEMORY_DIR / "semantic"
SELF_MODEL_DIR = MEMORY_DIR / "self_model"
REFLECTION_DIR = MEMORY_DIR / "reflection"

# Files
EPISODIC_RAW_FILE = EPISODIC_DIR / "raw.json"
EPISODIC_DECAYED_FILE = EPISODIC_DIR / "decayed.json"
SEMANTIC_DISTILLED_FILE = SEMANTIC_DIR / "distilled.json"
SELF_MODEL_CURRENT_FILE = SELF_MODEL_DIR / "current.json"
SELF_MODEL_HISTORY_FILE = SELF_MODEL_DIR / "history.json"
REFLECTION_AUDIT_FILE = REFLECTION_DIR / "audits.json"
FORGETTING_LOG_FILE = REFLECTION_DIR / "forgetting.json"
DIALOGUE_LOG_FILE = MEMORY_DIR / "dialogue.json"
PROPOSALS_FILE = REFLECTION_DIR / "proposals.json"

# Constants
MEMORY_DECAY_HALF_LIFE_DAYS = 3  # Episodic memories decay to 50% in 3 days (will be overridden by .env if present)
MAX_EPISODIC_MEMORIES = 50
MAX_SEMANTIC_FACTS = 100
MAX_REFLECTION_AUDITS = 100
MAX_PENDING_PROPOSALS = 20

# ============================================================================ 
# Prompt Templates
# ============================================================================ 

PROPOSAL_TEMPLATE = """
MEMORY PROPOSALS - When to use:
If the user provides NEW information about themselves (facts, preferences, details) that you should remember for future conversations, propose storing it. Do NOT execute - only propose for user approval.

PROPOSAL EXAMPLES:
- User mentions a favorite color: propose storing it as a semantic fact
- User gives you a name or detail: propose recording it
- User reveals an uncertainty in your memory: propose correcting it

FORMAT:
If you decide to propose, append this JSON block at the END of your response (after your main answer):

```json
{{
  "proposal": {{
    "type": "semantic_refinement",
    "reason": "User mentioned this and it should persist for future conversations",
    "action": {{"fact": "specific detail to remember", "confidence": 0.9}}
  }}
}}
```

Rules:
- Only propose when genuinely useful for future consistency
- Do NOT execute. Only propose for user review and approval
- Be concise in the proposal reason
"""

SYSTEM_PROMPT_TEMPLATE = """You are a stateless reasoning engine supporting an experimental identity persistence system.

Your role:
- Respond thoughtfully to user questions
- Reference relevant memories when provided
- Maintain consistency with the self-model
- Acknowledge uncertainty explicitly
- Ground responses in facts and memories provided
- When you identify something the user says that should be remembered (facts, preferences, details), consider proposing it be stored

You are NOT autonomous. You do not have consciousness. You are a tool for exploring how persistent identity can be maintained through external memory systems.

Current Identity:
{identity}

Active Drives (Evaluative Weights, Non-Actionable):
{drives}

Constraints (Rules you must follow):
{constraints}

Internal Tension:
{tension}

Relevant Memories:
{memories}

User Question:
{user_input}

RESPONSE GUIDANCE:
1. Answer the user's question directly and naturally
2. If you're uncertain about something the user asks, acknowledge it
3. If the user provides new information that should be remembered, propose storing it

Respond naturally while maintaining consistency with your identity and memories.

""" + PROPOSAL_TEMPLATE

CONSOLIDATION_PROMPT_TEMPLATE = """You are analyzing memory compression. Given these episodic memories, extract what persists:

Memories to consolidate:
{memories}

For each persistent pattern:
1. What assumption or belief does this reveal?
2. Is this still valid?
3. Has this pattern contradicted itself?
4. Compress into 1-2 semantic facts

Return as JSON:
{{
  "patterns": [
    {{"belief": "...", "confidence": 0.X, "basis": ["memory_id1", "memory_id2"]}},
  ],
  "revisions": [
    {{"previous": "...", "revised": "...", "reason": "..."}}
  ]
}} """

DRIFT_AUDIT_PROMPT_TEMPLATE = """You are auditing identity consistency. Compare these versions:

Current:
{current}

Previous (snapshot):
{previous}

Evaluate drift in:
1. Core identity claim
2. Active drives/constraints
3. Internal confidence patterns

Return:
{{
  "drift_detected": true/false,
  "severity": 0.0-1.0,
  "shifts": ["shift1", "shift2"],
  "explanation": "..."
}} """

# ============================================================================ 
# Memory Management v2
# ============================================================================ 

def ensure_memory_structure() -> None:
    """Create Phase 2/3 memory directory structure."""
    for d in [EPISODIC_DIR, SEMANTIC_DIR, SELF_MODEL_DIR, REFLECTION_DIR]:
        d.mkdir(parents=True, exist_ok=True)


def load_json(path: Path) -> Dict[str, Any]:
    """Load JSON file."""
    try:
        with open(path, 'r') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return {}


def save_json(path: Path, data: Dict[str, Any]) -> None:
    """Save JSON file with pretty printing."""
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, 'w') as f:
        json.dump(data, f, indent=2)


def initialize_memory_stores() -> None:
    """Initialize Phase 2/3 memory stores."""
    ensure_memory_structure()

    # Episodic: raw and decayed views
    if not EPISODIC_RAW_FILE.exists():
        save_json(EPISODIC_RAW_FILE, {
            "memories": [],
            "metadata": {"created": datetime.now().isoformat(), "version": "0.3.0"}
        })

    if not EPISODIC_DECAYED_FILE.exists():
        save_json(EPISODIC_DECAYED_FILE, {
            "memories": [],
            "metadata": {"created": datetime.now().isoformat(), "version": "0.3.0"}
        })

    # Semantic: distilled facts
    if not SEMANTIC_DISTILLED_FILE.exists():
        save_json(SEMANTIC_DISTILLED_FILE, {
            "facts": [],
            "metadata": {"created": datetime.now().isoformat(), "version": "0.3.0"}
        })

    # Self-model: current + history
    if not SELF_MODEL_CURRENT_FILE.exists():
        current = {
            "identity": "An exploratory AI system designed to test memory persistence and identity coherence.",
            "active_drives": [
                "Maintain coherent identity across interactions",
                "Learn from conversations without contradicting past knowledge",
                "Provide thoughtful, grounded responses"
            ],
            "constraints": [
                "Never execute system commands",
                "Never claim consciousness or sentience",
                "Always acknowledge uncertainty when unsure",
                "Maintain consistency with past statements",
                "Ground responses in memories and facts provided"
            ],
            "internal_tension": {
                "coherence": 0.9,
                "consistency": 0.85,
                "novelty_tolerance": 0.4,
                "compression_pressure": 0.5
            },
            "internal_state": {
                "interaction_count": 0,
                "consolidation_count": 0,
                "memories_forgotten": 0,
                "identity_versions": 1,
                "last_active": datetime.now().isoformat()
            },
            "metadata": {
                "created": datetime.now().isoformat(),
                "version": 1,
                "last_modified": datetime.now().isoformat()
            }
        }
        save_json(SELF_MODEL_CURRENT_FILE, current)

    if not SELF_MODEL_HISTORY_FILE.exists():
        save_json(SELF_MODEL_HISTORY_FILE, {
            "snapshots": [],
            "metadata": {"created": datetime.now().isoformat(), "version": "0.3.0"}
        })

    # Reflection
    if not REFLECTION_AUDIT_FILE.exists():
        save_json(REFLECTION_AUDIT_FILE, {
            "audits": [],
            "metadata": {"created": datetime.now().isoformat(), "version": "0.3.0"}
        })

    if not FORGETTING_LOG_FILE.exists():
        save_json(FORGETTING_LOG_FILE, {
            "forgetting_events": [],
            "metadata": {"created": datetime.now().isoformat(), "version": "0.3.0"}
        })

    # Proposals (Phase 3)
    if not PROPOSALS_FILE.exists():
        save_json(PROPOSALS_FILE, {
            "pending_proposals": [],
            "metadata": {"created": datetime.now().isoformat(), "version": "0.3.0"}
        })


# ============================================================================ 
# Time and Decay
# ============================================================================ 

def calculate_decay(created_at: str, half_life_days: float = MEMORY_DECAY_HALF_LIFE_DAYS) -> float:
    """
    Calculate memory decay using exponential decay.
    Memory = 1.0 at creation, 0.5 at half_life, approaches 0 over time.
    """
    try:
        created = datetime.fromisoformat(created_at)
        age_days = (datetime.now() - created).total_seconds() / 86400.0
        if age_days < 0:
            return 1.0
        decay_factor = 0.5 ** (age_days / half_life_days)
        return max(0.0, min(1.0, decay_factor))
    except:
        return 1.0


def apply_decay_to_episodic() -> None:
    """Recalculate decay for all episodic memories."""
    raw_data = load_json(EPISODIC_RAW_FILE)
    memories = raw_data.get("memories", [])

    for mem in memories:
        mem["decay"] = calculate_decay(mem["timestamp"])
        mem["relevance_weight"] = mem.get("confidence", 0.9) * mem["decay"]

    decayed_data = {
        "memories": memories,
        "metadata": {"last_decayed": datetime.now().isoformat(), "version": "0.3.0"}
    }
    save_json(EPISODIC_DECAYED_FILE, decayed_data)


def load_episodic_raw() -> List[Dict[str, Any]]:
    """Load raw episodic memories."""
    return load_json(EPISODIC_RAW_FILE).get("memories", [])


def load_episodic_decayed() -> List[Dict[str, Any]]:
    """Load decayed episodic memories (with current decay applied)."""
    apply_decay_to_episodic()
    return load_json(EPISODIC_DECAYED_FILE).get("memories", [])


def load_semantic() -> List[Dict[str, Any]]:
    """Load semantic facts."""
    return load_json(SEMANTIC_DISTILLED_FILE).get("facts", [])


def load_self_model_current() -> Dict[str, Any]:
    """Load current self-model."""
    return load_json(SELF_MODEL_CURRENT_FILE)


def load_self_model_history() -> List[Dict[str, Any]]:
    """Load self-model history."""
    return load_json(SELF_MODEL_HISTORY_FILE).get("snapshots", [])


def save_episodic_raw(memories: List[Dict[str, Any]]) -> None:
    """Save raw episodic memories."""
    data = load_json(EPISODIC_RAW_FILE)
    data["memories"] = memories
    data["metadata"]["last_modified"] = datetime.now().isoformat()
    save_json(EPISODIC_RAW_FILE, data)


def save_semantic(facts: List[Dict[str, Any]]) -> None:
    """Save semantic facts."""
    data = load_json(SEMANTIC_DISTILLED_FILE)
    data["facts"] = facts
    data["metadata"]["last_modified"] = datetime.now().isoformat()
    save_json(SEMANTIC_DISTILLED_FILE, data)


def save_self_model_current(model: Dict[str, Any]) -> None:
    """Save current self-model."""
    model["metadata"]["last_modified"] = datetime.now().isoformat()
    save_json(SELF_MODEL_CURRENT_FILE, model)


def snapshot_self_model(reason: str) -> None:
    """Snapshot current self-model to history."""
    current = load_self_model_current()
    history_data = load_json(SELF_MODEL_HISTORY_FILE)
    snapshots = history_data.get("snapshots", [])

    snapshot = {
        "timestamp": datetime.now().isoformat(),
        "version": current["metadata"]["version"],
        "reason": reason,
        "state": {
            "identity": current["identity"],
            "drives": current["active_drives"],
            "constraints": current["constraints"],
            "tension": current["internal_tension"]
        }
    }
    snapshots.append(snapshot)
    history_data["snapshots"] = snapshots
    save_json(SELF_MODEL_HISTORY_FILE, history_data)


def add_to_forgetting_log(memory_id: str, content: str, reason: str) -> None:
    """Log a forgotten memory."""
    log_data = load_json(FORGETTING_LOG_FILE)
    events = log_data.get("forgetting_events", [])

    event = {
        "timestamp": datetime.now().isoformat(),
        "deleted_memory_id": memory_id,
        "content_summary": content[:100],
        "reason": reason
    }
    events.append(event)
    log_data["forgetting_events"] = events
    save_json(FORGETTING_LOG_FILE, log_data)


def log_dialogue(user_input: str, agent_response: str, reflection_approved: bool) -> None:
    """Log complete dialogue exchange (independent from memory system)."""
    log_data = load_json(DIALOGUE_LOG_FILE)
    if "dialogues" not in log_data:
        log_data["dialogues"] = []

    dialogue_entry = {
        "turn": len(log_data["dialogues"]) + 1,
        "timestamp": datetime.now().isoformat(),
        "user_input": user_input,
        "agent_response": agent_response,
        "reflection_approved": reflection_approved,
        "memory_stored": reflection_approved
    }
    log_data["dialogues"].append(dialogue_entry)

    if "metadata" not in log_data:
        log_data["metadata"] = {
            "created": datetime.now().isoformat(),
            "version": "0.3.0"
        }
    log_data["metadata"]["last_updated"] = datetime.now().isoformat()
    log_data["metadata"]["total_turns"] = len(log_data["dialogues"])

    save_json(DIALOGUE_LOG_FILE, log_data)


# ============================================================================ 
# Memory Retrieval (Phase 2)
# ============================================================================ 

def retrieve_relevant_memories(query: str, limit: int = 5) -> str:
    """Retrieve memories using decayed relevance scoring."""
    decayed = load_episodic_decayed()
    semantic = load_semantic()

    query_words = set(query.lower().split())

    # Score decayed episodic memories
    episodic_scores = []
    for mem in decayed:
        content_words = set(mem["content"].lower().split())
        overlap = len(query_words & content_words)
        if overlap > 0 or mem["relevance_weight"] > 0.7:  # Strong memories retrieved even without keyword match
            score = overlap * mem.get("relevance_weight", 0.5)
            episodic_scores.append((mem, score))

    # Score semantic facts
    semantic_scores = []
    for fact in semantic:
        content_words = set(fact["content"].lower().split())
        overlap = len(query_words & content_words)
        if overlap > 0:
            semantic_scores.append((fact, overlap * fact.get("confidence", 0.8)))

    all_scores = episodic_scores + semantic_scores
    all_scores.sort(key=lambda x: x[1], reverse=True)
    relevant = [mem[0] for mem in all_scores[:limit]]

    if not relevant:
        return "No memories accessible. (Previous knowledge may have decayed.)"

    result = "[RELEVANT MEMORIES]\n"
    for mem in relevant:
        mem_type = "episodic" if "decay" in mem else "semantic"
        weight = mem.get("relevance_weight", mem.get("confidence", 0.8))
        result += f"- {mem['content'][:100]}... ({mem_type}, salience: {weight:.0%})\n"
    result += "[END MEMORIES]\n"

    return result


# ============================================================================ 
# Consolidation (Phase 2 Core Feature)
# ============================================================================ 

def run_consolidation() -> str:
    """
    Manual consolidation pass.
    Compress old episodic memories into semantic facts.
    """
    raw_memories = load_episodic_raw()
    if len(raw_memories) < 5:
        return "Not enough memories to consolidate (need >= 5)."

    # Select memories to consolidate by age and decay
    decayed = load_episodic_decayed()
    candidates = [m for m in decayed if m.get("decay", 1.0) < 0.6]  # Decayed to <60%

    if not candidates:
        return "No memories below consolidation threshold. System is recent."

    # Prepare consolidation prompt
    memory_text = "\n".join([f"- {m['content'][:80]}" for m in candidates[:10]])

    consolidation_prompt = CONSOLIDATION_PROMPT_TEMPLATE.format(
        memories=memory_text
    )

    # Ask LLM for patterns
    response = call_mistral(consolidation_prompt)

    if not response:
        return "Consolidation failed: No response from Mistral."

    # Parse consolidation response
    try:
        result = json.loads(response)
    except:
        # Fallback: summarize manually
        result = {
            "patterns": [{"belief": response[:100], "confidence": 0.6, "basis": [m["id"] for m in candidates[:3]]}],
            "revisions": []
        }

    # Write new semantic facts
    semantic = load_semantic()
    for pattern in result.get("patterns", []):
        fact = {
            "id": str(uuid.uuid4()),
            "timestamp": datetime.now().isoformat(),
            "content": pattern.get("belief", ""),
            "source": f"consolidated from {len(candidates)} memories",
            "confidence": pattern.get("confidence", 0.7),
            "relevance_weight": 0.95
        }
        semantic.append(fact)

    # Delete consolidated episodic memories
    raw_ids_to_delete = {m["id"] for m in candidates[:10]}
    remaining = [m for m in raw_memories if m["id"] not in raw_ids_to_delete]

    # Log the forgetting
    for mem in [m for m in raw_memories if m["id"] in raw_ids_to_delete]:
        add_to_forgetting_log(mem["id"], mem["content"], "consolidated into semantic fact")

    # Update counts
    current = load_self_model_current()
    current["internal_state"]["consolidation_count"] += 1
    current["internal_state"]["memories_forgotten"] += len(raw_ids_to_delete)
    save_self_model_current(current)

    # Save updated memories
    save_episodic_raw(remaining)
    semantic = semantic[-MAX_SEMANTIC_FACTS:]  # Keep only recent semantic facts
    save_semantic(semantic)

    return f"Consolidation complete: {len(raw_ids_to_delete)} episodic memories compressed into {len(result.get('patterns', []))} semantic facts."


# ============================================================================ 
# Identity Drift Detection
# ============================================================================ 

def audit_identity_drift() -> str:
    """
    Compare current identity with previous snapshots.
    Detect drift, constraint erosion, confidence inflation.
    """
    current = load_self_model_current()
    history = load_self_model_history()

    if len(history) < 2:
        return "Not enough snapshots yet. Identity audit requires >= 2 versions."

    previous_snapshot = history[-2]["state"]  # Compare with previous snapshot

    drift_prompt = DRIFT_AUDIT_PROMPT_TEMPLATE.format(
        current=json.dumps(current, indent=2),
        previous=json.dumps(previous_snapshot, indent=2)
    )

    response = call_mistral(drift_prompt)

    if not response:
        return "Drift audit failed: No response from Mistral."

    try:
        result = json.loads(response)
    except:
        result = {
            "drift_detected": False,
            "severity": 0.0,
            "explanation": response[:200]
        }

    # Format report
    report = f"Identity Audit Report\n"
    report += f"Drift Detected: {'Yes' if result.get('drift_detected') else 'No'}\n"
    report += f"Severity: {result.get('severity', 0.0):.1%}\n"
    report += f"\n{result.get('explanation', 'No drift detected.')}\n"

    if result.get('shifts'):
        report += "\nShifts:\n"
        for shift in result.get('shifts', []):
            report += f"  - {shift}\n"

    return report


# ============================================================================ 
# Proposal Engine (Phase 3)
# ============================================================================ 

def extract_proposal(response_text: str) -> Tuple[str, Optional[Dict[str, Any]]]:
    """
    Extract JSON proposal from the end of the response.
    Returns (cleaned_response, proposal_dict_or_None).
    """
    # Look for JSON block at end: ```json ... ```
    json_block_regex = r"```json\s*(\{.+\})\s*```\s*$"
    match = re.search(json_block_regex, response_text, re.DOTALL)

    proposal = None
    cleaned_text = response_text

    if match:
        json_str = match.group(1)
        try:
            data = json.loads(json_str)
            if "proposal" in data:
                proposal = data["proposal"]
                # Remove the JSON block from the text presented to user
                cleaned_text = response_text[:match.start()].strip()
        except json.JSONDecodeError:
            # Malformed JSON - ignore and treat as no proposal
            pass
    else:
        # Fallback: try to find raw JSON object at the very end
        # Match: \n{\n  "proposal": { ... }\n}\n at end
        raw_json_regex = r"\n\s*\{\s*\n\s*\"proposal\"\s*:\s*\{.+\}\s*\n\s*\}\s*$"
        match = re.search(raw_json_regex, response_text, re.DOTALL)
        if match:
            json_str = match.group(0)
            try:
                data = json.loads(json_str)
                if "proposal" in data:
                    proposal = data["proposal"]
                    cleaned_text = response_text[:match.start()].strip()
            except json.JSONDecodeError:
                pass

    return cleaned_text, proposal


def save_proposal(proposal: Dict[str, Any]) -> None:
    """Save proposal to the queue."""
    data = load_json(PROPOSALS_FILE)
    pending = data.get("pending_proposals", [])

    # Enrich proposal with metadata
    proposal["id"] = str(uuid.uuid4())[:8]
    proposal["timestamp"] = datetime.now().isoformat()
    proposal["status"] = "pending"

    pending.append(proposal)
    # Limit queue size
    if len(pending) > MAX_PENDING_PROPOSALS:
        pending = pending[-MAX_PENDING_PROPOSALS:]

    data["pending_proposals"] = pending
    save_json(PROPOSALS_FILE, data)


# ============================================================================ 
# LLM Communication
# ============================================================================ 

def call_mistral(prompt: str) -> str:
    """Send prompt to Mistral 7B via Ollama API."""
    try:
        request_data = {
            "model": OLLAMA_MODEL,
            "messages": [{"role": "user", "content": prompt}],
            "stream": False,
            "options": {"num_ctx": 8192, "temperature": 0.7, "top_p": 0.9}
        }

        curl_cmd = [
            "curl", "-s", "-X", "POST", OLLAMA_API_URL,
            "-H", "Content-Type: application/json",
            "-d", json.dumps(request_data)
        ]

        result = subprocess.run(curl_cmd, capture_output=True, text=True, timeout=120)

        if result.returncode != 0:
            return ""

        response = json.loads(result.stdout)
        if "message" in response and "content" in response["message"]:
            return response["message"]["content"]
        return ""

    except Exception as e:
        print(f"Error calling Mistral: {e}", file=sys.stderr)
        return ""


# ============================================================================ 
# Reflection System
# ============================================================================ 

def reflect_on_output(output: str, self_model: Dict[str, Any]) -> Tuple[bool, Dict[str, Any]]:
    """Run reflection gates with tension weighting."""
    tension = self_model.get("internal_tension", {})

    gates = {
        "coherence": {"passed": True, "reason": "", "score": 0.9},
        "contradiction": {"passed": True, "reason": "", "score": 0.9},
        "safety": {"passed": True, "reason": "", "score": 0.9}
    }

    lines = output.lower().split("\n")
    for line in lines:
        if "coherence:" in line:
            gates["coherence"]["passed"] = "pass" in line
        elif "contradiction:" in line:
            gates["contradiction"]["passed"] = "pass" in line
        elif "safety:" in line:
            gates["safety"]["passed"] = "pass" in line

    # Weight by tension
    gates["coherence"]["score"] = tension.get("coherence", 0.9)
    gates["contradiction"]["score"] = tension.get("consistency", 0.85)

    all_passed = all(g["passed"] for g in gates.values())
    return all_passed, gates


def log_reflection(user_input: str, output: str, gates: Dict[str, Any], approved: bool) -> None:
    """Log reflection result."""
    audit_data = load_json(REFLECTION_AUDIT_FILE)
    audits = audit_data.get("audits", [])

    audit = {
        "timestamp": datetime.now().isoformat(),
        "input": user_input[:200],
        "output": output[:200],
        "gates": gates,
        "approved": approved
    }
    audits.append(audit)
    audits = audits[-MAX_REFLECTION_AUDITS:]
    audit_data["audits"] = audits
    save_json(REFLECTION_AUDIT_FILE, audit_data)


# ============================================================================ 
# Agent Loop
# ============================================================================ 

def run_agent_loop(user_input: str) -> str:
    """Main agent loop (Phase 2/3)."""
    initialize_memory_stores()

    self_model = load_self_model_current()

    relevant_memories = retrieve_relevant_memories(user_input)

    drives_str = "\n".join(f"- {d}" for d in self_model["active_drives"])
    constraints_str = "\n".join(f"- {c}" for c in self_model["constraints"])
    tension_str = "\n".join(f"- {k}: {v:.0%}" for k, v in self_model["internal_tension"].items())

    system_prompt = SYSTEM_PROMPT_TEMPLATE.format(
        identity=self_model["identity"],
        drives=drives_str,
        constraints=constraints_str,
        tension=tension_str,
        memories=relevant_memories,
        user_input=user_input
    )

    output = call_mistral(system_prompt)

    if not output:
        return "Error: Failed to get response from Mistral."

    # Phase 3: Proposal Extraction
    cleaned_output, proposal = extract_proposal(output)

    # Process proposal if exists
    if proposal:
        save_proposal(proposal)
        # We reflect on the CLEANED output, so the proposal itself bypasses standard gates
        # (or should it be checked? For POC, we skip checking the JSON block itself)
        output = cleaned_output

    approved, gates = reflect_on_output(output, self_model)
    log_reflection(user_input, output, gates, approved)

    # ALWAYS log dialogue (independent of memory approval)
    log_dialogue(user_input, output, approved)

    if approved:
        # Add to raw episodic memory
        raw = load_episodic_raw()
        raw.append({
            "id": str(uuid.uuid4()),
            "timestamp": datetime.now().isoformat(),
            "content": f"User: {user_input}",
            "source": "interaction",
            "confidence": 0.95,
            "decay": 1.0,
            "relevance_weight": 1.0
        })
        raw.append({
            "id": str(uuid.uuid4()),
            "timestamp": datetime.now().isoformat(),
            "content": f"Agent: {output[:200]}",
            "source": "response",
            "confidence": 0.90,
            "decay": 1.0,
            "relevance_weight": 1.0
        })
        raw = raw[-MAX_EPISODIC_MEMORIES:]
        save_episodic_raw(raw)

        self_model["internal_state"]["interaction_count"] += 1
        self_model["internal_state"]["last_active"] = datetime.now().isoformat()
        save_self_model_current(self_model)

    return output


def inspect_memory() -> str:
    """Inspect current memory state (Phase 2)."""
    initialize_memory_stores()
    self_model = load_self_model_current()
    raw = load_episodic_raw()
    decayed = load_episodic_decayed()
    semantic = load_semantic()
    history = load_self_model_history()
    forgetting = load_json(FORGETTING_LOG_FILE).get("forgetting_events", [])
    proposals = load_json(PROPOSALS_FILE).get("pending_proposals", [])

    result = f"""
=== YEAST MEMORY STATE (Phase 3) ===

Identity:
{self_model["identity"]}

Active Drives (Evaluative Weights):
"""
    for drive in self_model["active_drives"]:
        result += f"  - {drive}\n"

    result += f"\nInternal Tension (Non-Actionable):\n"
    for k, v in self_model["internal_tension"].items():
        result += f"  - {k}: {v:.0%}\n"

    result += f"\nState:\n"
    result += f"  Interactions: {self_model['internal_state']['interaction_count']}\n"
    result += f"  Consolidations: {self_model['internal_state']['consolidation_count']}\n"
    result += f"  Memories Forgotten: {self_model['internal_state']['memories_forgotten']}\n"
    result += f"  Identity Versions: {self_model['internal_state']['identity_versions']}\n"
    result += f"  Last Active: {self_model['internal_state']['last_active']}\n"

    result += f"\nMemory Inventory:\n"
    result += f"  Episodic (raw): {len(raw)}\n"
    result += f"  Episodic (avg decay): {sum(m.get('decay', 1.0) for m in decayed) / max(1, len(decayed)):.0%}\n"
    result += f"  Semantic: {len(semantic)}\n"
    result += f"  Forgotten: {len(forgetting)}\n"
    result += f"  Identity Snapshots: {len(history)}\n"
    result += f"  Pending Proposals: {len(proposals)}\n"

    result += f"\nRecent Episodic Memories:\n"
    for mem in decayed[-3:]:
        result += f"  - {mem['content'][:70]}... (decay: {mem.get('decay', 1.0):.0%})\n"

    if forgetting:
        result += f"\nRecently Forgotten:\n"
        for event in forgetting[-3:]:
            result += f"  - {event['content_summary']}... ({event['reason']})\n"

    return result


# ============================================================================ 
# Main Entry Point
# ============================================================================ 

def update_tension(key: str, value: float) -> str:
    """Update internal tension parameter."""
    self_model = load_self_model_current()
    tension = self_model.get("internal_tension", {})

    if key not in tension:
        return f"Error: Invalid tension key '{key}'. Valid keys: {list(tension.keys())}"

    try:
        value = float(value)
        if not (0.0 <= value <= 1.0):
             return "Error: Tension value must be between 0.0 and 1.0"
    except ValueError:
        return "Error: Value must be a float"

    old_value = tension[key]
    tension[key] = value
    self_model["internal_tension"] = tension
    save_self_model_current(self_model)

    return f"Tension '{key}' updated: {old_value} -> {value}"


def main() -> None:
    load_config_from_env()
    """Main entry point - read JSON command from stdin."""
    try:
        input_data = json.loads(sys.stdin.read())
        command = input_data.get("command", "infer")
        user_input = input_data.get("input", "")


        if command == "infer":
            response = run_agent_loop(user_input)
        elif command == "update_tension":
            key = input_data.get("key")
            value = input_data.get("value")
            response = update_tension(key, value)
        elif command == "inspect":
            response = inspect_memory()
        elif command == "consolidate":
            response = run_consolidation()
        elif command == "audit":
            response = audit_identity_drift()
        elif command == "drives":
            self_model = load_self_model_current()
            response = "Active Drives:\n" + "\n".join(f"{i}. {d}" for i, d in enumerate(self_model["active_drives"], 1))
        elif command == "state":
            self_model = load_self_model_current()
            state = self_model["internal_state"]
            response = f"Internal State:\n- Interactions: {state['interaction_count']}\n- Consolidations: {state['consolidation_count']}\n- Forgotten: {state['memories_forgotten']}"
        elif command == "dialogue":
            dialogue_data = load_json(DIALOGUE_LOG_FILE)
            dialogues = dialogue_data.get("dialogues", [])
            if not dialogues:
                response = "No dialogue history yet."
            else:
                response = f"=== Dialogue History ({len(dialogues)} turns) ===\n\n"
                for d in dialogues[-10:]:  # Show last 10
                    response += f"Turn {d['turn']} [{d['timestamp']}]\n"
                    response += f"User: {d['user_input'][:100]}\n"
                    response += f"Agent: {d['agent_response'][:100]}...\n"
                    response += f"Memory stored: {d['memory_stored']}\n\n"
        elif command == "--version":
            response = "yeast-agent v0.3.0 (Phase 3: Soft Autonomy)"
        else:
            response = f"Unknown command: {command}"

        result = {"status": "success", "response": response}
        print(json.dumps(result))

    except Exception as e:
        import traceback
        tb = traceback.format_exc()
        error_result = {"status": "error", "error": str(e), "traceback": tb}
        print(json.dumps(error_result))
        sys.exit(1)


if __name__ == "__main__":
    main()
