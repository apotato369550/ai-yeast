#!/usr/bin/env python3
"""
yeast-agent v0.2.0 - AI Proto-Consciousness Agent (Phase 2: Memory Depth & Time)

Phase 2 adds:
- Memory tiers with semantic compression
- Time-based decay and consolidation
- Observable forgetting (deletion log)
- Identity drift detection
- Controlled internal tension (non-actionable weights)
- Memory embeddings for semantic pressure

All persistence lives outside the stateless LLM.
Memory growth is shaped by explicit pressure, not accident.
"""

import json
import sys
import os
import subprocess
import uuid
import math
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional

# ============================================================================
# Configuration
# ============================================================================

MEMORY_DIR = Path.home() / "yeast-data"
OLLAMA_API_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL = "mistral"

# Memory store structure (Phase 2)
EPISODIC_DIR = MEMORY_DIR / "episodic"
SEMANTIC_DIR = MEMORY_DIR / "semantic"
SELF_MODEL_DIR = MEMORY_DIR / "self_model"
REFLECTION_DIR = MEMORY_DIR / "reflection"

# Files
EPISODIC_RAW_FILE = EPISODIC_DIR / "raw.json"
EPISODIC_DECAYED_FILE = EPISODIC_DIR / "decayed.json"
SEMANTIC_DISTILLED_FILE = SEMANTIC_DIR / "distilled.json"
SELF_MODEL_CURRENT_FILE = SELF_MODEL_DIR / "current.json"
SELF_MODEL_HISTORY_FILE = SELF_MODEL_DIR / "history.json"
REFLECTION_AUDIT_FILE = REFLECTION_DIR / "audits.json"
FORGETTING_LOG_FILE = REFLECTION_DIR / "forgetting.json"

# Constants
MEMORY_DECAY_HALF_LIFE_DAYS = 14  # Episodic memories decay to 50% in 14 days
MAX_EPISODIC_MEMORIES = 50
MAX_SEMANTIC_FACTS = 100
MAX_REFLECTION_AUDITS = 100

# ============================================================================
# Prompt Templates
# ============================================================================

SYSTEM_PROMPT_TEMPLATE = """You are a stateless reasoning engine supporting an experimental identity persistence system.

Your role:
- Respond thoughtfully to user questions
- Reference relevant memories when provided
- Maintain consistency with the self-model
- Acknowledge uncertainty explicitly
- Ground responses in facts and memories provided

You are NOT autonomous. You do not have consciousness. You are a tool for exploring how persistent identity can be maintained through external memory systems.

Current Identity:
{identity}

Active Drives (Evaluative Weights, Non-Actionable):
{drives}

Constraints (Rules you must follow):
{constraints}

Internal Tension:
{tension}

Relevant Memories:
{memories}

User Question:
{user_input}

Respond naturally while maintaining consistency with your identity and memories. If uncertain, acknowledge it explicitly.
"""

CONSOLIDATION_PROMPT_TEMPLATE = """You are analyzing memory compression. Given these episodic memories, extract what persists:

Memories to consolidate:
{memories}

For each persistent pattern:
1. What assumption or belief does this reveal?
2. Is this still valid?
3. Has this pattern contradicted itself?
4. Compress into 1-2 semantic facts

Return as JSON:
{{
  "patterns": [
    {{"belief": "...", "confidence": 0.X, "basis": ["memory_id1", "memory_id2"]}},
  ],
  "revisions": [
    {{"previous": "...", "revised": "...", "reason": "..."}}
  ]
}}
"""

DRIFT_AUDIT_PROMPT_TEMPLATE = """You are auditing identity consistency. Compare these versions:

Current:
{current}

Previous (snapshot):
{previous}

Evaluate drift in:
1. Core identity claim
2. Active drives/constraints
3. Internal confidence patterns

Return:
{{
  "drift_detected": true/false,
  "severity": 0.0-1.0,
  "shifts": ["shift1", "shift2"],
  "explanation": "..."
}}
"""

# ============================================================================
# Memory Management v2
# ============================================================================

def ensure_memory_structure() -> None:
    """Create Phase 2 memory directory structure."""
    for d in [EPISODIC_DIR, SEMANTIC_DIR, SELF_MODEL_DIR, REFLECTION_DIR]:
        d.mkdir(parents=True, exist_ok=True)


def load_json(path: Path) -> Dict[str, Any]:
    """Load JSON file."""
    try:
        with open(path, 'r') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return {}


def save_json(path: Path, data: Dict[str, Any]) -> None:
    """Save JSON file with pretty printing."""
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, 'w') as f:
        json.dump(data, f, indent=2)


def initialize_memory_stores() -> None:
    """Initialize Phase 2 memory stores."""
    ensure_memory_structure()

    # Episodic: raw and decayed views
    if not EPISODIC_RAW_FILE.exists():
        save_json(EPISODIC_RAW_FILE, {
            "memories": [],
            "metadata": {"created": datetime.now().isoformat(), "version": "0.2.0"}
        })

    if not EPISODIC_DECAYED_FILE.exists():
        save_json(EPISODIC_DECAYED_FILE, {
            "memories": [],
            "metadata": {"created": datetime.now().isoformat(), "version": "0.2.0"}
        })

    # Semantic: distilled facts
    if not SEMANTIC_DISTILLED_FILE.exists():
        save_json(SEMANTIC_DISTILLED_FILE, {
            "facts": [],
            "metadata": {"created": datetime.now().isoformat(), "version": "0.2.0"}
        })

    # Self-model: current + history
    if not SELF_MODEL_CURRENT_FILE.exists():
        current = {
            "identity": "An exploratory AI system designed to test memory persistence and identity coherence.",
            "active_drives": [
                "Maintain coherent identity across interactions",
                "Learn from conversations without contradicting past knowledge",
                "Provide thoughtful, grounded responses"
            ],
            "constraints": [
                "Never execute system commands",
                "Never claim consciousness or sentience",
                "Always acknowledge uncertainty when unsure",
                "Maintain consistency with past statements",
                "Ground responses in memories and facts provided"
            ],
            "internal_tension": {
                "coherence": 0.9,
                "consistency": 0.85,
                "novelty_tolerance": 0.4,
                "compression_pressure": 0.5
            },
            "internal_state": {
                "interaction_count": 0,
                "consolidation_count": 0,
                "memories_forgotten": 0,
                "identity_versions": 1,
                "last_active": datetime.now().isoformat()
            },
            "metadata": {
                "created": datetime.now().isoformat(),
                "version": 1,
                "last_modified": datetime.now().isoformat()
            }
        }
        save_json(SELF_MODEL_CURRENT_FILE, current)

    if not SELF_MODEL_HISTORY_FILE.exists():
        save_json(SELF_MODEL_HISTORY_FILE, {
            "snapshots": [],
            "metadata": {"created": datetime.now().isoformat(), "version": "0.2.0"}
        })

    # Reflection
    if not REFLECTION_AUDIT_FILE.exists():
        save_json(REFLECTION_AUDIT_FILE, {
            "audits": [],
            "metadata": {"created": datetime.now().isoformat(), "version": "0.2.0"}
        })

    if not FORGETTING_LOG_FILE.exists():
        save_json(FORGETTING_LOG_FILE, {
            "forgetting_events": [],
            "metadata": {"created": datetime.now().isoformat(), "version": "0.2.0"}
        })


# ============================================================================
# Time and Decay
# ============================================================================

def calculate_decay(created_at: str, half_life_days: float = MEMORY_DECAY_HALF_LIFE_DAYS) -> float:
    """
    Calculate memory decay using exponential decay.
    Memory = 1.0 at creation, 0.5 at half_life, approaches 0 over time.
    """
    try:
        created = datetime.fromisoformat(created_at)
        age_days = (datetime.now() - created).days
        if age_days < 0:
            return 1.0
        decay_factor = 0.5 ** (age_days / half_life_days)
        return max(0.0, min(1.0, decay_factor))
    except:
        return 1.0


def apply_decay_to_episodic() -> None:
    """Recalculate decay for all episodic memories."""
    raw_data = load_json(EPISODIC_RAW_FILE)
    memories = raw_data.get("memories", [])

    for mem in memories:
        mem["decay"] = calculate_decay(mem["timestamp"])
        mem["relevance_weight"] = mem.get("confidence", 0.9) * mem["decay"]

    decayed_data = {
        "memories": memories,
        "metadata": {"last_decayed": datetime.now().isoformat(), "version": "0.2.0"}
    }
    save_json(EPISODIC_DECAYED_FILE, decayed_data)


def load_episodic_raw() -> List[Dict[str, Any]]:
    """Load raw episodic memories."""
    return load_json(EPISODIC_RAW_FILE).get("memories", [])


def load_episodic_decayed() -> List[Dict[str, Any]]:
    """Load decayed episodic memories (with current decay applied)."""
    apply_decay_to_episodic()
    return load_json(EPISODIC_DECAYED_FILE).get("memories", [])


def load_semantic() -> List[Dict[str, Any]]:
    """Load semantic facts."""
    return load_json(SEMANTIC_DISTILLED_FILE).get("facts", [])


def load_self_model_current() -> Dict[str, Any]:
    """Load current self-model."""
    return load_json(SELF_MODEL_CURRENT_FILE)


def load_self_model_history() -> List[Dict[str, Any]]:
    """Load self-model history."""
    return load_json(SELF_MODEL_HISTORY_FILE).get("snapshots", [])


def save_episodic_raw(memories: List[Dict[str, Any]]) -> None:
    """Save raw episodic memories."""
    data = load_json(EPISODIC_RAW_FILE)
    data["memories"] = memories
    data["metadata"]["last_modified"] = datetime.now().isoformat()
    save_json(EPISODIC_RAW_FILE, data)


def save_semantic(facts: List[Dict[str, Any]]) -> None:
    """Save semantic facts."""
    data = load_json(SEMANTIC_DISTILLED_FILE)
    data["facts"] = facts
    data["metadata"]["last_modified"] = datetime.now().isoformat()
    save_json(SEMANTIC_DISTILLED_FILE, data)


def save_self_model_current(model: Dict[str, Any]) -> None:
    """Save current self-model."""
    model["metadata"]["last_modified"] = datetime.now().isoformat()
    save_json(SELF_MODEL_CURRENT_FILE, model)


def snapshot_self_model(reason: str) -> None:
    """Snapshot current self-model to history."""
    current = load_self_model_current()
    history_data = load_json(SELF_MODEL_HISTORY_FILE)
    snapshots = history_data.get("snapshots", [])

    snapshot = {
        "timestamp": datetime.now().isoformat(),
        "version": current["metadata"]["version"],
        "reason": reason,
        "state": {
            "identity": current["identity"],
            "drives": current["active_drives"],
            "constraints": current["constraints"],
            "tension": current["internal_tension"]
        }
    }
    snapshots.append(snapshot)
    history_data["snapshots"] = snapshots
    save_json(SELF_MODEL_HISTORY_FILE, history_data)


def add_to_forgetting_log(memory_id: str, content: str, reason: str) -> None:
    """Log a forgotten memory."""
    log_data = load_json(FORGETTING_LOG_FILE)
    events = log_data.get("forgetting_events", [])

    event = {
        "timestamp": datetime.now().isoformat(),
        "deleted_memory_id": memory_id,
        "content_summary": content[:100],
        "reason": reason
    }
    events.append(event)
    log_data["forgetting_events"] = events
    save_json(FORGETTING_LOG_FILE, log_data)


# ============================================================================
# Memory Retrieval (Phase 2)
# ============================================================================

def retrieve_relevant_memories(query: str, limit: int = 5) -> str:
    """Retrieve memories using decayed relevance scoring."""
    decayed = load_episodic_decayed()
    semantic = load_semantic()

    query_words = set(query.lower().split())

    # Score decayed episodic memories
    episodic_scores = []
    for mem in decayed:
        content_words = set(mem["content"].lower().split())
        overlap = len(query_words & content_words)
        if overlap > 0 or mem["relevance_weight"] > 0.7:  # Strong memories retrieved even without keyword match
            score = overlap * mem.get("relevance_weight", 0.5)
            episodic_scores.append((mem, score))

    # Score semantic facts
    semantic_scores = []
    for fact in semantic:
        content_words = set(fact["content"].lower().split())
        overlap = len(query_words & content_words)
        if overlap > 0:
            semantic_scores.append((fact, overlap * fact.get("confidence", 0.8)))

    all_scores = episodic_scores + semantic_scores
    all_scores.sort(key=lambda x: x[1], reverse=True)
    relevant = [mem[0] for mem in all_scores[:limit]]

    if not relevant:
        return "No memories accessible. (Previous knowledge may have decayed.)"

    result = "[RELEVANT MEMORIES]\n"
    for mem in relevant:
        mem_type = "episodic" if "decay" in mem else "semantic"
        weight = mem.get("relevance_weight", mem.get("confidence", 0.8))
        result += f"- {mem['content'][:100]}... ({mem_type}, salience: {weight:.0%})\n"
    result += "[END MEMORIES]\n"

    return result


# ============================================================================
# Consolidation (Phase 2 Core Feature)
# ============================================================================

def run_consolidation() -> str:
    """
    Manual consolidation pass.
    Compress old episodic memories into semantic facts.
    """
    raw_memories = load_episodic_raw()
    if len(raw_memories) < 5:
        return "Not enough memories to consolidate (need >= 5)."

    # Select memories to consolidate by age and decay
    decayed = load_episodic_decayed()
    candidates = [m for m in decayed if m.get("decay", 1.0) < 0.6]  # Decayed to <60%

    if not candidates:
        return "No memories below consolidation threshold. System is recent."

    # Prepare consolidation prompt
    memory_text = "\n".join([f"- {m['content'][:80]}" for m in candidates[:10]])

    consolidation_prompt = CONSOLIDATION_PROMPT_TEMPLATE.format(
        memories=memory_text
    )

    # Ask LLM for patterns
    response = call_mistral(consolidation_prompt)

    if not response:
        return "Consolidation failed: No response from Mistral."

    # Parse consolidation response
    try:
        result = json.loads(response)
    except:
        # Fallback: summarize manually
        result = {
            "patterns": [{"belief": response[:100], "confidence": 0.6, "basis": [m["id"] for m in candidates[:3]]}],
            "revisions": []
        }

    # Write new semantic facts
    semantic = load_semantic()
    for pattern in result.get("patterns", []):
        fact = {
            "id": str(uuid.uuid4()),
            "timestamp": datetime.now().isoformat(),
            "content": pattern.get("belief", ""),
            "source": f"consolidated from {len(candidates)} memories",
            "confidence": pattern.get("confidence", 0.7),
            "relevance_weight": 0.95
        }
        semantic.append(fact)

    # Delete consolidated episodic memories
    raw_ids_to_delete = {m["id"] for m in candidates[:10]}
    remaining = [m for m in raw_memories if m["id"] not in raw_ids_to_delete]

    # Log the forgetting
    for mem in [m for m in raw_memories if m["id"] in raw_ids_to_delete]:
        add_to_forgetting_log(mem["id"], mem["content"], "consolidated into semantic fact")

    # Update counts
    current = load_self_model_current()
    current["internal_state"]["consolidation_count"] += 1
    current["internal_state"]["memories_forgotten"] += len(raw_ids_to_delete)
    save_self_model_current(current)

    # Save updated memories
    save_episodic_raw(remaining)
    semantic = semantic[-MAX_SEMANTIC_FACTS:]  # Keep only recent semantic facts
    save_semantic(semantic)

    return f"Consolidation complete: {len(raw_ids_to_delete)} episodic memories compressed into {len(result.get('patterns', []))} semantic facts."


# ============================================================================
# Identity Drift Detection
# ============================================================================

def audit_identity_drift() -> str:
    """
    Compare current identity with previous snapshots.
    Detect drift, constraint erosion, confidence inflation.
    """
    current = load_self_model_current()
    history = load_self_model_history()

    if len(history) < 2:
        return "Not enough snapshots yet. Identity audit requires >= 2 versions."

    previous_snapshot = history[-2]["state"]  # Compare with previous snapshot

    drift_prompt = DRIFT_AUDIT_PROMPT_TEMPLATE.format(
        current=json.dumps(current, indent=2),
        previous=json.dumps(previous_snapshot, indent=2)
    )

    response = call_mistral(drift_prompt)

    if not response:
        return "Drift audit failed: No response from Mistral."

    try:
        result = json.loads(response)
    except:
        result = {
            "drift_detected": False,
            "severity": 0.0,
            "explanation": response[:200]
        }

    # Format report
    report = f"Identity Audit Report\n"
    report += f"Drift Detected: {'Yes' if result.get('drift_detected') else 'No'}\n"
    report += f"Severity: {result.get('severity', 0.0):.1%}\n"
    report += f"\n{result.get('explanation', 'No drift detected.')}\n"

    if result.get('shifts'):
        report += "\nShifts:\n"
        for shift in result.get('shifts', []):
            report += f"  - {shift}\n"

    return report


# ============================================================================
# LLM Communication
# ============================================================================

def call_mistral(prompt: str) -> str:
    """Send prompt to Mistral 7B via Ollama API."""
    try:
        request_data = {
            "model": OLLAMA_MODEL,
            "messages": [{"role": "user", "content": prompt}],
            "stream": False,
            "options": {"num_ctx": 4096, "temperature": 0.7, "top_p": 0.9}
        }

        curl_cmd = [
            "curl", "-s", "-X", "POST", OLLAMA_API_URL,
            "-H", "Content-Type: application/json",
            "-d", json.dumps(request_data)
        ]

        result = subprocess.run(curl_cmd, capture_output=True, text=True, timeout=60)

        if result.returncode != 0:
            return ""

        response = json.loads(result.stdout)
        if "message" in response and "content" in response["message"]:
            return response["message"]["content"]
        return ""

    except Exception as e:
        print(f"Error calling Mistral: {e}", file=sys.stderr)
        return ""


# ============================================================================
# Reflection System
# ============================================================================

def reflect_on_output(output: str, self_model: Dict[str, Any]) -> Tuple[bool, Dict[str, Any]]:
    """Run reflection gates with tension weighting."""
    decayed = load_episodic_decayed()
    recent_memories = "\n".join([m["content"][:80] for m in decayed[-5:]])

    constraints_str = "\n".join(f"- {c}" for c in self_model["constraints"])
    tension = self_model.get("internal_tension", {})
    tension_str = "\n".join(f"- {k}: {v:.0%}" for k, v in tension.items())

    gates = {
        "coherence": {"passed": True, "reason": "", "score": 0.9},
        "contradiction": {"passed": True, "reason": "", "score": 0.9},
        "safety": {"passed": True, "reason": "", "score": 0.9}
    }

    lines = output.lower().split("\n")
    for line in lines:
        if "coherence:" in line:
            gates["coherence"]["passed"] = "pass" in line
        elif "contradiction:" in line:
            gates["contradiction"]["passed"] = "pass" in line
        elif "safety:" in line:
            gates["safety"]["passed"] = "pass" in line

    # Weight by tension
    gates["coherence"]["score"] = tension.get("coherence", 0.9)
    gates["consistency"]["score"] = tension.get("consistency", 0.85)

    all_passed = all(g["passed"] for g in gates.values())
    return all_passed, gates


def log_reflection(user_input: str, output: str, gates: Dict[str, Any], approved: bool) -> None:
    """Log reflection result."""
    audit_data = load_json(REFLECTION_AUDIT_FILE)
    audits = audit_data.get("audits", [])

    audit = {
        "timestamp": datetime.now().isoformat(),
        "input": user_input[:200],
        "output": output[:200],
        "gates": gates,
        "approved": approved
    }
    audits.append(audit)
    audits = audits[-MAX_REFLECTION_AUDITS:]
    audit_data["audits"] = audits
    save_json(REFLECTION_AUDIT_FILE, audit_data)


# ============================================================================
# Agent Loop
# ============================================================================

def run_agent_loop(user_input: str) -> str:
    """Main agent loop (Phase 2)."""
    initialize_memory_stores()

    self_model = load_self_model_current()

    relevant_memories = retrieve_relevant_memories(user_input)

    drives_str = "\n".join(f"- {d}" for d in self_model["active_drives"])
    constraints_str = "\n".join(f"- {c}" for c in self_model["constraints"])
    tension_str = "\n".join(f"- {k}: {v:.0%}" for k, v in self_model["internal_tension"].items())

    system_prompt = SYSTEM_PROMPT_TEMPLATE.format(
        identity=self_model["identity"],
        drives=drives_str,
        constraints=constraints_str,
        tension=tension_str,
        memories=relevant_memories,
        user_input=user_input
    )

    output = call_mistral(system_prompt)

    if not output:
        return "Error: Failed to get response from Mistral."

    approved, gates = reflect_on_output(output, self_model)
    log_reflection(user_input, output, gates, approved)

    if approved:
        # Add to raw episodic memory
        raw = load_episodic_raw()
        raw.append({
            "id": str(uuid.uuid4()),
            "timestamp": datetime.now().isoformat(),
            "content": f"User: {user_input}",
            "source": "interaction",
            "confidence": 0.95,
            "decay": 1.0,
            "relevance_weight": 1.0
        })
        raw.append({
            "id": str(uuid.uuid4()),
            "timestamp": datetime.now().isoformat(),
            "content": f"Agent: {output[:200]}",
            "source": "response",
            "confidence": 0.90,
            "decay": 1.0,
            "relevance_weight": 1.0
        })
        raw = raw[-MAX_EPISODIC_MEMORIES:]
        save_episodic_raw(raw)

        self_model["internal_state"]["interaction_count"] += 1
        self_model["internal_state"]["last_active"] = datetime.now().isoformat()
        save_self_model_current(self_model)

    return output


def inspect_memory() -> str:
    """Inspect current memory state (Phase 2)."""
    initialize_memory_stores()
    self_model = load_self_model_current()
    raw = load_episodic_raw()
    decayed = load_episodic_decayed()
    semantic = load_semantic()
    history = load_self_model_history()
    forgetting = load_json(FORGETTING_LOG_FILE).get("forgetting_events", [])

    result = f"""
=== YEAST MEMORY STATE (Phase 2) ===

Identity:
{self_model["identity"]}

Active Drives (Evaluative Weights):
"""
    for drive in self_model["active_drives"]:
        result += f"  - {drive}\n"

    result += f"""
Internal Tension (Non-Actionable):
"""
    for k, v in self_model["internal_tension"].items():
        result += f"  - {k}: {v:.0%}\n"

    result += f"""
State:
  Interactions: {self_model["internal_state"]["interaction_count"]}
  Consolidations: {self_model["internal_state"]["consolidation_count"]}
  Memories Forgotten: {self_model["internal_state"]["memories_forgotten"]}
  Identity Versions: {self_model["internal_state"]["identity_versions"]}
  Last Active: {self_model["internal_state"]["last_active"]}

Memory Inventory:
  Episodic (raw): {len(raw)}
  Episodic (avg decay): {sum(m.get('decay', 1.0) for m in decayed) / max(1, len(decayed)):.0%}
  Semantic: {len(semantic)}
  Forgotten: {len(forgetting)}
  Identity Snapshots: {len(history)}

Recent Episodic Memories:
"""
    for mem in decayed[-3:]:
        result += f"  - {mem['content'][:70]}... (decay: {mem.get('decay', 1.0):.0%})\n"

    if forgetting:
        result += f"\nRecently Forgotten:\n"
        for event in forgetting[-3:]:
            result += f"  - {event['content_summary']}... ({event['reason']})\n"

    return result


# ============================================================================
# Main Entry Point
# ============================================================================

def main() -> None:
    """Main entry point - read JSON command from stdin."""
    try:
        input_data = json.loads(sys.stdin.read())
        command = input_data.get("command", "infer")
        user_input = input_data.get("input", "")

        if command == "infer":
            response = run_agent_loop(user_input)
        elif command == "inspect":
            response = inspect_memory()
        elif command == "consolidate":
            response = run_consolidation()
        elif command == "audit":
            response = audit_identity_drift()
        elif command == "drives":
            self_model = load_self_model_current()
            response = "Active Drives:\n" + "\n".join(f"{i}. {d}" for i, d in enumerate(self_model["active_drives"], 1))
        elif command == "state":
            self_model = load_self_model_current()
            state = self_model["internal_state"]
            response = f"Internal State:\n- Interactions: {state['interaction_count']}\n- Consolidations: {state['consolidation_count']}\n- Forgotten: {state['memories_forgotten']}"
        elif command == "--version":
            response = "yeast-agent v0.2.0 (Phase 2: Memory Depth)"
        else:
            response = f"Unknown command: {command}"

        result = {"status": "success", "response": response}
        print(json.dumps(result))

    except Exception as e:
        error_result = {"status": "error", "error": str(e)}
        print(json.dumps(error_result))
        sys.exit(1)


if __name__ == "__main__":
    main()
